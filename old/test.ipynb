{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': {'n_epochs': 20, 'batch_size': 8, 'learning_rate': 0.001, 'checkpoint_directory': './chk', 'input_channels': 3, 'numer_classes': 6, 'segmentation_classes': 5, 'filters': 32, 'dropout': 0.1, 'feature_dimensions': 128, 'optimiser': 'Adam', 'weight_decay': 0.0001}, 'dataset': {'path': '/gluster/data/dune/niclane/nlane_prod_strange_resample_fhc_run2_fhc_reco2_reco2_trainingimage_signal_lambdamuon_100_ana.root', 'tree': 'imageanalyser/ImageTree', 'width': 512, 'height': 512, 'planes': ['U', 'V', 'W'], 'induction_plane': 2}}\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_get_random', 'visualise_input_event', 'visualise_overlay_event', 'visualise_truth_event']\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "from src.config import Config\n",
    "from src.dataset import Dataset \n",
    "from src.visualiser import Visualiser\n",
    "from src.models import SimCLRModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import importlib\n",
    "import src.visualiser as visualiser_module\n",
    "importlib.reload(visualiser_module)\n",
    "from src.visualiser import Visualiser\n",
    "\n",
    "pth = \"cfg/default.yaml\"\n",
    "cfg = Config(pth)\n",
    "\n",
    "print(cfg.as_dict())\n",
    "print(dir(Visualiser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(cfg)\n",
    "vis = Visualiser(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_loader = DataLoader(ds, batch_size=16, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimCLRModel(in_channels=1, feature_dim=128, projection_hidden_dim=512, projection_dim=128).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_positive_info_nce_loss(features, num_views=3, temperature=0.5):\n",
    "    features = F.normalize(features, dim=1)\n",
    "    batch_size = features.shape[0] // num_views\n",
    "    loss = 0.0\n",
    "    total_count = 0\n",
    "    similarity_matrix = torch.matmul(features, features.T)\n",
    "    for i in range(batch_size):\n",
    "        indices = torch.arange(i * num_views, (i + 1) * num_views, device=features.device)\n",
    "        for anchor in indices:\n",
    "            positives = indices[indices != anchor]\n",
    "            pos_sim = torch.exp(similarity_matrix[anchor, positives] / temperature).sum()\n",
    "            mask = torch.ones(features.shape[0], dtype=torch.bool, device=features.device)\n",
    "            mask[indices] = False\n",
    "            neg_sim = torch.exp(similarity_matrix[anchor][mask] / temperature).sum()\n",
    "            loss += -torch.log(pos_sim / (pos_sim + neg_sim))\n",
    "            total_count += 1\n",
    "    return loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 0\n",
      "Batch 0: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 0: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 0: Projections shape = torch.Size([48, 128])\n",
      "Batch 0: Loss = 3.1506\n",
      "Batch 1: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 1: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 1: Projections shape = torch.Size([48, 128])\n",
      "Batch 1: Loss = 3.0020\n",
      "Batch 2: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 2: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 2: Projections shape = torch.Size([48, 128])\n",
      "Batch 2: Loss = 2.7980\n",
      "Batch 3: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 3: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 3: Projections shape = torch.Size([48, 128])\n",
      "Batch 3: Loss = 3.0182\n",
      "Batch 4: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 4: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 4: Projections shape = torch.Size([48, 128])\n",
      "Batch 4: Loss = 3.0607\n",
      "Batch 5: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 5: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 5: Projections shape = torch.Size([48, 128])\n",
      "Batch 5: Loss = 2.9015\n",
      "Batch 6: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 6: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 6: Projections shape = torch.Size([48, 128])\n",
      "Batch 6: Loss = 2.9180\n",
      "Batch 7: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 7: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 7: Projections shape = torch.Size([48, 128])\n",
      "Batch 7: Loss = 3.0126\n",
      "Batch 8: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 8: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 8: Projections shape = torch.Size([48, 128])\n",
      "Batch 8: Loss = 2.9738\n",
      "Batch 9: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 9: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 9: Projections shape = torch.Size([48, 128])\n",
      "Batch 9: Loss = 3.0487\n",
      "Batch 10: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 10: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 10: Projections shape = torch.Size([48, 128])\n",
      "Batch 10: Loss = 3.0939\n",
      "Batch 11: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 11: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 11: Projections shape = torch.Size([48, 128])\n",
      "Batch 11: Loss = 3.0017\n",
      "Batch 12: Original shape = torch.Size([16, 3, 512, 512])\n",
      "Batch 12: Reshaped to torch.Size([48, 1, 512, 512])\n",
      "Batch 12: Projections shape = torch.Size([48, 128])\n",
      "Batch 12: Loss = 2.9937\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    print(\"Starting epoch\", epoch)\n",
    "    for batch_idx, (images, _, _, _, _) in enumerate(train_loader):\n",
    "        B, planes, H, W = images.shape\n",
    "        print(f\"Batch {batch_idx}: Original shape = {images.shape}\")\n",
    "        if planes < 3:\n",
    "            print(f\"Batch {batch_idx}: Skipped because number of planes ({planes}) is less than 3\")\n",
    "            continue\n",
    "        images = images.view(B * 3, 1, H, W).to(device)\n",
    "        print(f\"Batch {batch_idx}: Reshaped to {images.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        projections = model(images)\n",
    "        print(f\"Batch {batch_idx}: Projections shape = {projections.shape}\")\n",
    "        loss = multi_positive_info_nce_loss(projections, num_views=3, temperature=0.5)\n",
    "        print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n",
    "    print(f\"Epoch {epoch}: Average Loss = {avg_loss:.4f}\")\n",
    "torch.save(model.encoder.state_dict(), \"pretrained_encoder.pth\")\n",
    "print(\"Pretrained encoder saved as 'pretrained_encoder.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Class values must be smaller than num_classes.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/gluster/home/niclane/scanningforstrangeness/test.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bnoether/gluster/home/niclane/scanningforstrangeness/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m vis\u001b[39m.\u001b[39;49mvisualise_input_event(ds)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bnoether/gluster/home/niclane/scanningforstrangeness/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m vis\u001b[39m.\u001b[39mvisualise_input_event(ds)\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bnoether/gluster/home/niclane/scanningforstrangeness/test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m vis\u001b[39m.\u001b[39mvisualise_input_event(ds)\n",
      "File \u001b[0;32m~/scanningforstrangeness/src/visualiser.py:23\u001b[0m, in \u001b[0;36mVisualiser.visualise_input_event\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualise_input_event\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[1;32m     22\u001b[0m     idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_random(dataset)\n\u001b[0;32m---> 23\u001b[0m     event_data \u001b[39m=\u001b[39m dataset[idx]\n\u001b[1;32m     24\u001b[0m     input_img, _, r, sr, evnum \u001b[39m=\u001b[39m event_data\n\u001b[1;32m     25\u001b[0m     planes \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mU\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m][:input_img\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[0;32m~/scanningforstrangeness/src/dataset.py:49\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m plane_label \u001b[39min\u001b[39;00m targets_np:\n\u001b[1;32m     48\u001b[0m     plane_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(plane_label, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m---> 49\u001b[0m     one_hot \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mone_hot(plane_tensor, num_classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseg_classes)\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m     one_hot_targets\u001b[39m.\u001b[39mappend(one_hot)\n\u001b[1;32m     51\u001b[0m one_hot_targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(one_hot_targets, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Class values must be smaller than num_classes."
     ]
    }
   ],
   "source": [
    "vis.visualise_input_event(ds)\n",
    "vis.visualise_input_event(ds)\n",
    "vis.visualise_input_event(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)\n",
    "vis.visualise_overlay_event(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythondl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
