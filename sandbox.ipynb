{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "090b9e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2a3110e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['imageanalyser;1', 'imageanalyser/SampleTree;1', 'imageanalyser/ImageTree;1', 'FRH;1', 'FRV;1', 'rICKR;1', 'rICKI;1', 'PreC;1', 'PostC;1', 'PostO;1', 'PreD;1', 'PostDO;1', 'ER;1']\n"
     ]
    }
   ],
   "source": [
    "file_path = \"/gluster/data/dune/niclane/nlane_prod_strange_resample_fhc_run2_fhc_reco2_reco2_trainingimage_background_lambdamuon_ana.root\"  \n",
    "#file_path = \"/gluster/data/dune/niclane/test.root\"\n",
    "\n",
    "root_file = uproot.open(file_path)\n",
    "print(root_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d50cfe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'subrun', 'event', 'event_type', 'planes', 'width', 'height', 'input_data', 'truth_data']\n"
     ]
    }
   ],
   "source": [
    "tree_name = \"imageanalyser/ImageTree\" \n",
    "tree = root_file[tree_name]\n",
    "\n",
    "print(tree.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0598ffd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], ...]\n",
      "Data Type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "input_data = tree[\"input_data\"].array(library=\"np\")\n",
    "truth_data = tree[\"truth_data\"].array(library=\"np\")\n",
    "\n",
    "print(\"Shape:\", input_data[1])\n",
    "print(\"Data Type:\", type(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d3bc2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events: 706\n",
      "Planes per event: 3\n",
      "Pixels per plane: 262144\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of events: {len(input_data)}\")  \n",
    "print(f\"Planes per event: {len(input_data[0])}\")  \n",
    "print(f\"Pixels per plane: {len(input_data[0][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "530e9d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'uproot.containers.STLVector'>\n",
      "Number of entries in imageanalyser/ImageTree: 706\n"
     ]
    }
   ],
   "source": [
    "event_index = 2\n",
    "event = input_data[event_index]  \n",
    "truth_event = truth_data[event_index] \n",
    "\n",
    "run_numbers = tree[\"run\"].array(library=\"np\")\n",
    "subrun_numbers = tree[\"subrun\"].array(library=\"np\")\n",
    "event_numbers = tree[\"event\"].array(library=\"np\")\n",
    "\n",
    "event_type = tree[\"event_type\"].array(library=\"np\")\n",
    "\n",
    "print(event_type[event_index])\n",
    "\n",
    "print(type(input_data))\n",
    "plane = event[0]\n",
    "print(type(plane))\n",
    "\n",
    "num_entries = tree.num_entries\n",
    "print(f\"Number of entries in {tree_name}: {num_entries}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "852c01c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = 512, 512  \n",
    "\n",
    "plane_images = [np.array(list(plane), dtype=np.float32).reshape(H, W) for plane in event]\n",
    "truth_images = [np.array(list(plane), dtype=np.float32).reshape(H, W) for plane in truth_event]\n",
    "\n",
    "plane_labels = [\"U\", \"V\", \"W\"]\n",
    "\n",
    "r, sr, evnum = run_numbers[event_index], subrun_numbers[event_index], event_numbers[event_index]\n",
    "\n",
    "for i, (input_img, truth_img) in enumerate(zip(plane_images, truth_images)):\n",
    "    fig, ax = plt.subplots(figsize=(12, 12), dpi=600)\n",
    "\n",
    "    ax.imshow(input_img,\n",
    "              origin=\"lower\",\n",
    "              cmap=\"jet\",\n",
    "              norm=colors.PowerNorm(gamma=0.35, vmin=input_img.min(), vmax=input_img.max()))\n",
    "\n",
    "    overlay = False\n",
    "    if overlay:\n",
    "        ax.imshow(truth_img, origin=\"lower\", cmap=\"cool\", alpha=0.4)\n",
    "\n",
    "    ax.set_xticks([0, W - 1])\n",
    "    ax.set_yticks([0, H - 1])\n",
    "    ax.tick_params(axis=\"both\", direction=\"out\", length=6, width=1.5, labelsize=18)\n",
    "    ax.set_xlim(0, W - 1)\n",
    "    ax.set_ylim(0, H - 1)\n",
    "    ax.set_xlabel(\"Local Wire Coord\", fontsize=20)\n",
    "    ax.set_ylabel(\"Local Drift Time\", fontsize=20)\n",
    "    ax.set_title(f\"Plane {plane_labels[i]} (Run {r}, Subrun {sr}, Event {evnum})\", fontsize=22)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"event_{r}_{sr}_{evnum}_plane_{plane_labels[i]}.png\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06465db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Event Index: 5\n",
      "Event Type: 1\n",
      "Number of truth image channels: 3\n"
     ]
    }
   ],
   "source": [
    "event_indices = [5]\n",
    "\n",
    "run_numbers = tree[\"run\"].array(library=\"np\")\n",
    "subrun_numbers = tree[\"subrun\"].array(library=\"np\")\n",
    "event_numbers = tree[\"event\"].array(library=\"np\")\n",
    "event_type = tree[\"event_type\"].array(library=\"np\")\n",
    "\n",
    "W, H = 512, 512  \n",
    "plane_labels = [\"U\", \"V\", \"W\"]\n",
    "\n",
    "for event_index in event_indices:\n",
    "    event = input_data[event_index]  \n",
    "    truth_event = truth_data[event_index]  \n",
    "\n",
    "    print(f\"Processing Event Index: {event_index}\")\n",
    "    print(f\"Event Type: {event_type[event_index]}\")\n",
    "\n",
    "    r, sr, evnum = run_numbers[event_index], subrun_numbers[event_index], event_numbers[event_index]\n",
    "\n",
    "    plane_images = [np.array(list(plane), dtype=np.float32).reshape(H, W) for plane in event]\n",
    "    truth_images = [np.array(list(plane), dtype=np.float32).reshape(H, W) for plane in truth_event]\n",
    "\n",
    "    print(f\"Number of truth image channels: {len(truth_images)}\")\n",
    "\n",
    "    for i, (input_img, truth_img) in enumerate(zip(plane_images, truth_images)):\n",
    "        fig, ax = plt.subplots(figsize=(12, 12), dpi=600)\n",
    "\n",
    "        ax.imshow(input_img,\n",
    "                  origin=\"lower\",\n",
    "                  cmap=\"jet\",\n",
    "                  norm=colors.PowerNorm(gamma=0.35, vmin=input_img.min(), vmax=input_img.max()))\n",
    "\n",
    "        overlay = True\n",
    "        if overlay:\n",
    "            ax.imshow(truth_img, origin=\"lower\", cmap=\"cool\", alpha=0.4)\n",
    "\n",
    "        ax.set_xticks([0, W - 1])\n",
    "        ax.set_yticks([0, H - 1])\n",
    "        ax.tick_params(axis=\"both\", direction=\"out\", length=6, width=1.5, labelsize=18)\n",
    "        ax.set_xlim(0, W - 1)\n",
    "        ax.set_ylim(0, H - 1)\n",
    "        ax.set_xlabel(\"Local Wire Coord\", fontsize=20)\n",
    "        ax.set_ylabel(\"Local Drift Time\", fontsize=20)\n",
    "        ax.set_title(f\"Plane {plane_labels[i]} (Run {r}, Subrun {sr}, Event {evnum})\", fontsize=22)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"event_{r}_{sr}_{evnum}_plane_{plane_labels[i]}.png\")\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62f510e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.config import Config\n",
    "from src.dataset import SegmentationDataset, ContrastiveDataset  \n",
    "from src.visualiser import Visualiser\n",
    "from src.trainers import SegmentationTrainer, ContrastiveTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1823df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration:\n",
      "{'train': {'objective': 'classification', 'num_epochs': 20, 'batch_size': 8, 'lr': 0.001, 'ckpt_dir': './checkpoints', 'temp': 0.1}, 'model': {'arch': 'UResNetEncoder', 'in_channels': 3, 'num_classes': 6, 'seg_classes': 2, 'filters': 32, 'dropout': 0.1, 'feat_dim': 128, 'optimizer': 'Adam', 'weight_decay': 0.0001}, 'dataset': {'path': '/gluster/data/dune/niclane/nlane_prod_strange_resample_fhc_run2_fhc_reco2_reco2_trainingimage_background_lambdamuon_ana.root', 'tree': 'imageanalyser/ImageTree', 'dims': {'width': 512, 'height': 512}, 'planes': ['U', 'V', 'W'], 'ind_plane_idx': 2, 'filters': {'event_type': 'all'}}}\n",
      "Dataset File Path: None\n"
     ]
    }
   ],
   "source": [
    "path = \"cfg/default.yaml\"\n",
    "config = Config(path)\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(config.as_dict())\n",
    "print(\"Dataset File Path:\", config.get(\"dataset.file_path\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d573d4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegmentationDataset initialized with 706 samples.\n",
      "ContrastiveDataset initialized with 706 samples.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    segmentation_dataset = SegmentationDataset(config)\n",
    "    print(f\"SegmentationDataset initialized with {len(segmentation_dataset)} samples.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading SegmentationDataset: {e}\")\n",
    "\n",
    "try:\n",
    "    contrastive_dataset = ContrastiveDataset(config)\n",
    "    print(f\"ContrastiveDataset initialized with {len(contrastive_dataset)} samples.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ContrastiveDataset: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe194a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualiser = Visualiser(segmentation_dataset, width=512, height=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b08bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendering Event 10 | Plane U | Run 6369, Subrun 37, Event 1853\n",
      "Rendering Event 10 | Plane V | Run 6369, Subrun 37, Event 1853\n",
      "Rendering Event 10 | Plane W | Run 6369, Subrun 37, Event 1853\n"
     ]
    }
   ],
   "source": [
    "visualiser.visualise_event_planes(idx=10, save=True, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62cdac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Config:\n",
      " dataset:\n",
      "  dims:\n",
      "    height: 512\n",
      "    width: 512\n",
      "  filters:\n",
      "    event_type: all\n",
      "  ind_plane_idx: 2\n",
      "  path: /gluster/data/dune/niclane/nlane_prod_strange_resample_fhc_run2_fhc_reco2_reco2_trainingimage_background_lambdamuon_ana.root\n",
      "  planes:\n",
      "  - U\n",
      "  - V\n",
      "  - W\n",
      "  tree: imageanalyser/ImageTree\n",
      "model:\n",
      "  arch: UResNetEncoder\n",
      "  dropout: 0.1\n",
      "  feat_dim: 128\n",
      "  filters: 32\n",
      "  in_channels: 3\n",
      "  num_classes: 6\n",
      "  optimizer: Adam\n",
      "  seg_classes: 2\n",
      "  weight_decay: 0.0001\n",
      "train:\n",
      "  batch_size: 8\n",
      "  ckpt_dir: ./checkpoints\n",
      "  lr: 0.001\n",
      "  num_epochs: 20\n",
      "  objective: classification\n",
      "  temp: 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from src.config import Config\n",
    "\n",
    "config_path = \"cfg/default.yaml\"  \n",
    "config = Config(config_path)\n",
    "\n",
    "print(\"Training Config:\\n\", yaml.dump(config.as_dict(), default_flow_style=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78f2c292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gluster/home/niclane/scanningforstrangeness/src/trainers.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "dataset = SegmentationDataset(config)\n",
    "trainer = SegmentationTrainer(config, dataset)\n",
    "#print(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e996c273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** Epoch 1 Training\n",
      " Batch 000 | Images: torch.Size([8, 3, 512, 512]) | Labels: torch.Size([8, 6, 512, 512])\n",
      " Batch 000 | Outputs: torch.Size([8, 6, 512, 512])\n",
      " Batch 000 | Global Weights: [0.00099803 0.33281824 0.00099769 0.375521   0.00099849 0.28866652]\n",
      " Batch 000 | Loss: 0.8968\n",
      " Batch 001 | Images: torch.Size([8, 3, 512, 512]) | Labels: torch.Size([8, 6, 512, 512])\n",
      " Batch 001 | Outputs: torch.Size([8, 6, 512, 512])\n",
      " Batch 001 | Global Weights: [0.00080434 0.34593895 0.00080425 0.3628055  0.00080471 0.28884232]\n",
      " Batch 001 | Loss: 0.7837\n",
      " Batch 002 | Images: torch.Size([8, 3, 512, 512]) | Labels: torch.Size([8, 6, 512, 512])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/gluster/home/niclane/scanningforstrangeness/sandbox.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bnoether/gluster/home/niclane/scanningforstrangeness/sandbox.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/scanningforstrangeness/src/trainers.py:91\u001b[0m, in \u001b[0;36mSegmentationTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Batch \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Images: \u001b[39m\u001b[39m{\u001b[39;00mimages\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m | Labels: \u001b[39m\u001b[39m{\u001b[39;00mlabels\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 91\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(images)\n\u001b[1;32m     92\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Batch \u001b[39m\u001b[39m{\u001b[39;00mbatch_idx\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Outputs: \u001b[39m\u001b[39m{\u001b[39;00moutputs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m batch_counts \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msum(dim\u001b[39m=\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))\n",
      "File \u001b[0;32m~/miniforge3/envs/pythondl/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/pythondl/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scanningforstrangeness/src/models.py:102\u001b[0m, in \u001b[0;36mUResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    100\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds_maxpool_1(res)\n\u001b[1;32m    101\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds_dropout_1(res)\n\u001b[0;32m--> 102\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mds_conv_2(res)\n\u001b[1;32m    103\u001b[0m conv_stack_2 \u001b[39m=\u001b[39m res\u001b[39m.\u001b[39mclone()\n\u001b[1;32m    104\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds_maxpool_2(res)\n",
      "File \u001b[0;32m~/miniforge3/envs/pythondl/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/pythondl/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/scanningforstrangeness/src/models.py:37\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x)\n\u001b[1;32m     36\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x)\n\u001b[0;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x \u001b[39m+\u001b[39;49m identity)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a174047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_checkpoint(checkpoint_path, config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = UResNet(\n",
    "        in_dim=config.get(\"model.in_channels\"),\n",
    "        n_classes=config.get(\"model.num_classes\"),\n",
    "        n_filters=config.get(\"model.filters\"),\n",
    "        drop_prob=config.get(\"model.dropout\"),\n",
    "        y_range=None\n",
    "    )\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def run_inference(model, sample):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image, truth = sample\n",
    "    image_tensor = image.unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "    prediction = torch.argmax(output, dim=1).cpu().squeeze(0)\n",
    "    return image, truth, prediction\n",
    "\n",
    "def visualise_prediction(image, truth, prediction, plane, run, subrun, event):\n",
    "    fig, ax = plt.subplots(figsize=(12,12), dpi=600)\n",
    "    norm_img = mcolors.PowerNorm(gamma=0.35, vmin=image.min(), vmax=image.max())\n",
    "    ax.imshow(image, origin=\"lower\", cmap=\"jet\", norm=norm_img)\n",
    "    num_classes = int(np.max(prediction)) + 1\n",
    "    cmap_seg = plt.get_cmap(\"tab10\", num_classes)\n",
    "    boundaries = np.arange(-0.5, num_classes+0.5, 1)\n",
    "    norm_seg = mcolors.BoundaryNorm(boundaries, cmap_seg.N)\n",
    "    ax.imshow(prediction, origin=\"lower\", cmap=cmap_seg, norm=norm_seg, alpha=0.5)\n",
    "    ax.set_xticks([0, image.shape[1]-1])\n",
    "    ax.set_yticks([0, image.shape[0]-1])\n",
    "    ax.tick_params(axis=\"both\", direction=\"out\", length=6, width=1.5, labelsize=18)\n",
    "    ax.set_xlabel(\"Local Wire Coord\", fontsize=20)\n",
    "    ax.set_ylabel(\"Local Drift Time\", fontsize=20)\n",
    "    ax.set_title(f\"Plane {plane} | Run {run}, Subrun {subrun}, Event {event}\", fontsize=22)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"inference_plane_{plane}_run_{run}_subrun_{subrun}_event_{event}.png\")\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def visualise_model_inference(model, dataset, event_idx):\n",
    "    sample = dataset[event_idx]\n",
    "    image, truth, prediction = run_inference(model, sample)\n",
    "    run, subrun, event = \"Unknown\", \"Unknown\", event_idx\n",
    "    planes = [\"U\", \"V\", \"W\"]\n",
    "    for i, plane in enumerate(planes):\n",
    "        visualise_prediction(image[i], truth[i], prediction[i], plane, run, subrun, event)\n",
    "\n",
    "def plot_loss_curve(epoch_loss_history):\n",
    "    epochs = np.array([e[0] for e in epoch_loss_history])\n",
    "    train_loss = np.array([e[1] for e in epoch_loss_history])\n",
    "    train_err = np.array([e[2] for e in epoch_loss_history])\n",
    "    val_loss = np.array([e[3] for e in epoch_loss_history])\n",
    "    val_err = np.array([e[4] for e in epoch_loss_history])\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.errorbar(epochs, train_loss, yerr=train_err, fmt='-o', capsize=5, label='Train Loss', color='blue', markerfacecolor='white')\n",
    "    plt.errorbar(epochs, val_loss, yerr=val_err, fmt='-o', capsize=5, label='Validation Loss', color='orange', markerfacecolor='white')\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.title('Training and Validation Loss', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('loss_curve.png', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c34b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"checkpoints/SegmentationTrainer_epoch_1.pth\"\n",
    "model = load_model_checkpoint(checkpoint_path, config)\n",
    "dataset = SegmentationDataset(config)\n",
    "visualise_model_inference(model, dataset, event_idx=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae805d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0d221299",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import yaml\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "from src.config import Config\n",
    "from src.dataset import SegmentationDataset, ContrastiveDataset  \n",
    "from src.visualiser import Visualiser\n",
    "from src.trainers import SegmentationTrainer, ContrastiveTrainer\n",
    "\n",
    "import importlib\n",
    "import src.visualiser as visualiser_module\n",
    "importlib.reload(visualiser_module)\n",
    "from src.visualiser import Visualiser\n",
    "\n",
    "path = \"cfg/default.yaml\"\n",
    "config = Config(path)\n",
    "\n",
    "print(\"Loaded configuration:\")\n",
    "print(config.as_dict())\n",
    "\n",
    "print(dir(Visualiser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e5d595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 512, 512]) torch.Size([6, 512, 512])\n",
      "Saved: out/input_plane_U_event_0.png\n",
      "Saved: out/ground_truth_plane_U_event_0.png\n",
      "Saved: out/input_plane_V_event_0.png\n",
      "Saved: out/ground_truth_plane_V_event_0.png\n",
      "Saved: out/input_plane_W_event_0.png\n",
      "Saved: out/ground_truth_plane_W_event_0.png\n"
     ]
    }
   ],
   "source": [
    "dataset = SegmentationDataset(config)\n",
    "vis = Visualiser(config)\n",
    "sample = dataset[0]\n",
    "print(sample[0].shape, sample[1].shape)\n",
    "vis.visualise_event(dataset, event_index=0, model=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c9e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gluster/home/niclane/scanningforstrangeness/src/visualiser.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.eval()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: out/input_plane_U_event_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gluster/home/niclane/scanningforstrangeness/src/visualiser.py:50: RuntimeWarning: invalid value encountered in divide\n",
      "  return cmap, norm\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (512,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/gluster/home/niclane/scanningforstrangeness/sandbox.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bnoether/gluster/home/niclane/scanningforstrangeness/sandbox.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m checkpoint_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcheckpoints/SegmentationTrainer_epoch_1.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bnoether/gluster/home/niclane/scanningforstrangeness/sandbox.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m vis\u001b[39m.\u001b[39mload_model_checkpoint(checkpoint_path, UResNet, config)\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bnoether/gluster/home/niclane/scanningforstrangeness/sandbox.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m vis\u001b[39m.\u001b[39;49mvisualise_event(dataset, event_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, model\u001b[39m=\u001b[39;49mmodel)\n",
      "File \u001b[0;32m~/scanningforstrangeness/src/visualiser.py:76\u001b[0m, in \u001b[0;36mVisualiser.visualise_event\u001b[0;34m(self, dataset, event_index, model, overlay)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisualise_plane(image[i], gt_mask[i], \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mplane\u001b[39m}\u001b[39;00m\u001b[39m - Run \u001b[39m\u001b[39m{\u001b[39;00mrun_number\u001b[39m}\u001b[39;00m\u001b[39m | Subrun \u001b[39m\u001b[39m{\u001b[39;00msubrun_number\u001b[39m}\u001b[39;00m\u001b[39m | Event \u001b[39m\u001b[39m{\u001b[39;00mevent_number\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minput_plane_\u001b[39m\u001b[39m{\u001b[39;00mplane\u001b[39m}\u001b[39;00m\u001b[39m_event_\u001b[39m\u001b[39m{\u001b[39;00mevent_number\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m, overlay\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 76\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvisualise_plane(gt_mask[i], gt_mask[i], \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGround Truth \u001b[39;49m\u001b[39m{\u001b[39;49;00mplane\u001b[39m}\u001b[39;49;00m\u001b[39m - Run \u001b[39;49m\u001b[39m{\u001b[39;49;00mrun_number\u001b[39m}\u001b[39;49;00m\u001b[39m | Subrun \u001b[39;49m\u001b[39m{\u001b[39;49;00msubrun_number\u001b[39m}\u001b[39;49;00m\u001b[39m | Event \u001b[39;49m\u001b[39m{\u001b[39;49;00mevent_number\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mground_truth_plane_\u001b[39;49m\u001b[39m{\u001b[39;49;00mplane\u001b[39m}\u001b[39;49;00m\u001b[39m_event_\u001b[39;49m\u001b[39m{\u001b[39;49;00mevent_number\u001b[39m}\u001b[39;49;00m\u001b[39m.png\u001b[39;49m\u001b[39m\"\u001b[39;49m, overlay\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m prediction \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisualise_plane(prediction[i], prediction[i], \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrediction \u001b[39m\u001b[39m{\u001b[39;00mplane\u001b[39m}\u001b[39;00m\u001b[39m - Run \u001b[39m\u001b[39m{\u001b[39;00mrun_number\u001b[39m}\u001b[39;00m\u001b[39m | Subrun \u001b[39m\u001b[39m{\u001b[39;00msubrun_number\u001b[39m}\u001b[39;00m\u001b[39m | Event \u001b[39m\u001b[39m{\u001b[39;00mevent_number\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mprediction_plane_\u001b[39m\u001b[39m{\u001b[39;00mplane\u001b[39m}\u001b[39;00m\u001b[39m_event_\u001b[39m\u001b[39m{\u001b[39;00mevent_number\u001b[39m}\u001b[39;00m\u001b[39m.png\u001b[39m\u001b[39m\"\u001b[39m, overlay\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/scanningforstrangeness/src/visualiser.py:55\u001b[0m, in \u001b[0;36mVisualiser.visualise_plane\u001b[0;34m(self, image, mask, title, filename, overlay)\u001b[0m\n\u001b[1;32m     53\u001b[0m fig, ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(figsize\u001b[39m=\u001b[39m(\u001b[39m8\u001b[39m, \u001b[39m8\u001b[39m))\n\u001b[1;32m     54\u001b[0m norm_image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_normalise_image(image)\n\u001b[0;32m---> 55\u001b[0m ax\u001b[39m.\u001b[39;49mimshow(norm_image, origin\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlower\u001b[39;49m\u001b[39m\"\u001b[39;49m, cmap\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgray\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m overlay:\n\u001b[1;32m     57\u001b[0m     cmap \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mget_cmap(\u001b[39m\"\u001b[39m\u001b[39mtab10\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes)\n",
      "File \u001b[0;32m~/miniforge3/envs/pythondl/lib/python3.8/site-packages/matplotlib/__init__.py:1446\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1444\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1445\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1446\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1448\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1449\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1450\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/miniforge3/envs/pythondl/lib/python3.8/site-packages/matplotlib/axes/_axes.py:5663\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5656\u001b[0m im \u001b[39m=\u001b[39m mimage\u001b[39m.\u001b[39mAxesImage(\u001b[39mself\u001b[39m, cmap\u001b[39m=\u001b[39mcmap, norm\u001b[39m=\u001b[39mnorm,\n\u001b[1;32m   5657\u001b[0m                       interpolation\u001b[39m=\u001b[39minterpolation, origin\u001b[39m=\u001b[39morigin,\n\u001b[1;32m   5658\u001b[0m                       extent\u001b[39m=\u001b[39mextent, filternorm\u001b[39m=\u001b[39mfilternorm,\n\u001b[1;32m   5659\u001b[0m                       filterrad\u001b[39m=\u001b[39mfilterrad, resample\u001b[39m=\u001b[39mresample,\n\u001b[1;32m   5660\u001b[0m                       interpolation_stage\u001b[39m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5661\u001b[0m                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 5663\u001b[0m im\u001b[39m.\u001b[39;49mset_data(X)\n\u001b[1;32m   5664\u001b[0m im\u001b[39m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5665\u001b[0m \u001b[39mif\u001b[39;00m im\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5666\u001b[0m     \u001b[39m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/pythondl/lib/python3.8/site-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A[:, :, \u001b[39m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39min\u001b[39;00m [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m for image data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[39m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[39m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[39m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[39m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_A\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39minteger) \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (512,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.models import UResNet\n",
    "checkpoint_path = \"checkpoints/SegmentationTrainer_epoch_1.pth\"\n",
    "model = vis.load_model_checkpoint(checkpoint_path, UResNet, config)\n",
    "vis.visualise_event(dataset, event_index=0, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6949007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dafecc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
